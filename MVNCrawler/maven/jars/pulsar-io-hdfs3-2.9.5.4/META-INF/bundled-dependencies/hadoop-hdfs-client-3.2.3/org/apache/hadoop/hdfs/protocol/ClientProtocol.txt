Compiled from "ClientProtocol.java"
public interface org.apache.hadoop.hdfs.protocol.ClientProtocol {
  public static final long versionID;

  public static final int GET_STATS_CAPACITY_IDX;

  public static final int GET_STATS_USED_IDX;

  public static final int GET_STATS_REMAINING_IDX;

  public static final int GET_STATS_UNDER_REPLICATED_IDX;

  public static final int GET_STATS_LOW_REDUNDANCY_IDX;

  public static final int GET_STATS_CORRUPT_BLOCKS_IDX;

  public static final int GET_STATS_MISSING_BLOCKS_IDX;

  public static final int GET_STATS_MISSING_REPL_ONE_BLOCKS_IDX;

  public static final int GET_STATS_BYTES_IN_FUTURE_BLOCKS_IDX;

  public static final int GET_STATS_PENDING_DELETION_BLOCKS_IDX;

  public static final int STATS_ARRAY_LENGTH;

  public abstract org.apache.hadoop.hdfs.protocol.LocatedBlocks getBlockLocations(java.lang.String, long, long) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.FsServerDefaults getServerDefaults() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.HdfsFileStatus create(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, java.lang.String, org.apache.hadoop.io.EnumSetWritable<org.apache.hadoop.fs.CreateFlag>, boolean, short, long, org.apache.hadoop.crypto.CryptoProtocolVersion[], java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.LastBlockWithStatus append(java.lang.String, java.lang.String, org.apache.hadoop.io.EnumSetWritable<org.apache.hadoop.fs.CreateFlag>) throws java.io.IOException;

  public abstract boolean setReplication(java.lang.String, short) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.BlockStoragePolicy[] getStoragePolicies() throws java.io.IOException;

  public abstract void setStoragePolicy(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void unsetStoragePolicy(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.BlockStoragePolicy getStoragePolicy(java.lang.String) throws java.io.IOException;

  public abstract void setPermission(java.lang.String, org.apache.hadoop.fs.permission.FsPermission) throws java.io.IOException;

  public abstract void setOwner(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void abandonBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock, long, java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.LocatedBlock addBlock(java.lang.String, java.lang.String, org.apache.hadoop.hdfs.protocol.ExtendedBlock, org.apache.hadoop.hdfs.protocol.DatanodeInfo[], long, java.lang.String[], java.util.EnumSet<org.apache.hadoop.hdfs.AddBlockFlag>) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.LocatedBlock getAdditionalDatanode(java.lang.String, long, org.apache.hadoop.hdfs.protocol.ExtendedBlock, org.apache.hadoop.hdfs.protocol.DatanodeInfo[], java.lang.String[], org.apache.hadoop.hdfs.protocol.DatanodeInfo[], int, java.lang.String) throws java.io.IOException;

  public abstract boolean complete(java.lang.String, java.lang.String, org.apache.hadoop.hdfs.protocol.ExtendedBlock, long) throws java.io.IOException;

  public abstract void reportBadBlocks(org.apache.hadoop.hdfs.protocol.LocatedBlock[]) throws java.io.IOException;

  public abstract boolean rename(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void concat(java.lang.String, java.lang.String[]) throws java.io.IOException;

  public abstract void rename2(java.lang.String, java.lang.String, org.apache.hadoop.fs.Options$Rename...) throws java.io.IOException;

  public abstract boolean truncate(java.lang.String, long, java.lang.String) throws java.io.IOException;

  public abstract boolean delete(java.lang.String, boolean) throws java.io.IOException;

  public abstract boolean mkdirs(java.lang.String, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.DirectoryListing getListing(java.lang.String, byte[], boolean) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.SnapshottableDirectoryStatus[] getSnapshottableDirListing() throws java.io.IOException;

  public abstract void renewLease(java.lang.String) throws java.io.IOException;

  public abstract boolean recoverLease(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract long[] getStats() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.ReplicatedBlockStats getReplicatedBlockStats() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.ECBlockGroupStats getECBlockGroupStats() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.DatanodeInfo[] getDatanodeReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.server.protocol.DatanodeStorageReport[] getDatanodeStorageReport(org.apache.hadoop.hdfs.protocol.HdfsConstants$DatanodeReportType) throws java.io.IOException;

  public abstract long getPreferredBlockSize(java.lang.String) throws java.io.IOException;

  public abstract boolean setSafeMode(org.apache.hadoop.hdfs.protocol.HdfsConstants$SafeModeAction, boolean) throws java.io.IOException;

  public abstract boolean saveNamespace(long, long) throws java.io.IOException;

  public abstract long rollEdits() throws java.io.IOException;

  public abstract boolean restoreFailedStorage(java.lang.String) throws java.io.IOException;

  public abstract void refreshNodes() throws java.io.IOException;

  public abstract void finalizeUpgrade() throws java.io.IOException;

  public abstract boolean upgradeStatus() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.RollingUpgradeInfo rollingUpgrade(org.apache.hadoop.hdfs.protocol.HdfsConstants$RollingUpgradeAction) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.CorruptFileBlocks listCorruptFileBlocks(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void metaSave(java.lang.String) throws java.io.IOException;

  public abstract void setBalancerBandwidth(long) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.HdfsFileStatus getFileInfo(java.lang.String) throws java.io.IOException;

  public abstract boolean isFileClosed(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.HdfsFileStatus getFileLinkInfo(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus getLocatedFileInfo(java.lang.String, boolean) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.ContentSummary getContentSummary(java.lang.String) throws java.io.IOException;

  public abstract void setQuota(java.lang.String, long, long, org.apache.hadoop.fs.StorageType) throws java.io.IOException;

  public abstract void fsync(java.lang.String, long, java.lang.String, long) throws java.io.IOException;

  public abstract void setTimes(java.lang.String, long, long) throws java.io.IOException;

  public abstract void createSymlink(java.lang.String, java.lang.String, org.apache.hadoop.fs.permission.FsPermission, boolean) throws java.io.IOException;

  public abstract java.lang.String getLinkTarget(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.LocatedBlock updateBlockForPipeline(org.apache.hadoop.hdfs.protocol.ExtendedBlock, java.lang.String) throws java.io.IOException;

  public abstract void updatePipeline(java.lang.String, org.apache.hadoop.hdfs.protocol.ExtendedBlock, org.apache.hadoop.hdfs.protocol.ExtendedBlock, org.apache.hadoop.hdfs.protocol.DatanodeID[], java.lang.String[]) throws java.io.IOException;

  public abstract org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier> getDelegationToken(org.apache.hadoop.io.Text) throws java.io.IOException;

  public abstract long renewDelegationToken(org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier>) throws java.io.IOException;

  public abstract void cancelDelegationToken(org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier>) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.security.token.block.DataEncryptionKey getDataEncryptionKey() throws java.io.IOException;

  public abstract java.lang.String createSnapshot(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void deleteSnapshot(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void renameSnapshot(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract void allowSnapshot(java.lang.String) throws java.io.IOException;

  public abstract void disallowSnapshot(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.SnapshotDiffReport getSnapshotDiffReport(java.lang.String, java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.SnapshotDiffReportListing getSnapshotDiffReportListing(java.lang.String, java.lang.String, java.lang.String, byte[], int) throws java.io.IOException;

  public abstract long addCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo, java.util.EnumSet<org.apache.hadoop.fs.CacheFlag>) throws java.io.IOException;

  public abstract void modifyCacheDirective(org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo, java.util.EnumSet<org.apache.hadoop.fs.CacheFlag>) throws java.io.IOException;

  public abstract void removeCacheDirective(long) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.CacheDirectiveEntry> listCacheDirectives(long, org.apache.hadoop.hdfs.protocol.CacheDirectiveInfo) throws java.io.IOException;

  public abstract void addCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo) throws java.io.IOException;

  public abstract void modifyCachePool(org.apache.hadoop.hdfs.protocol.CachePoolInfo) throws java.io.IOException;

  public abstract void removeCachePool(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.CachePoolEntry> listCachePools(java.lang.String) throws java.io.IOException;

  public abstract void modifyAclEntries(java.lang.String, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;

  public abstract void removeAclEntries(java.lang.String, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;

  public abstract void removeDefaultAcl(java.lang.String) throws java.io.IOException;

  public abstract void removeAcl(java.lang.String) throws java.io.IOException;

  public abstract void setAcl(java.lang.String, java.util.List<org.apache.hadoop.fs.permission.AclEntry>) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.permission.AclStatus getAclStatus(java.lang.String) throws java.io.IOException;

  public abstract void createEncryptionZone(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.EncryptionZone getEZForPath(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.EncryptionZone> listEncryptionZones(long) throws java.io.IOException;

  public abstract void reencryptEncryptionZone(java.lang.String, org.apache.hadoop.hdfs.protocol.HdfsConstants$ReencryptAction) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.ZoneReencryptionStatus> listReencryptionStatus(long) throws java.io.IOException;

  public abstract void setXAttr(java.lang.String, org.apache.hadoop.fs.XAttr, java.util.EnumSet<org.apache.hadoop.fs.XAttrSetFlag>) throws java.io.IOException;

  public abstract java.util.List<org.apache.hadoop.fs.XAttr> getXAttrs(java.lang.String, java.util.List<org.apache.hadoop.fs.XAttr>) throws java.io.IOException;

  public abstract java.util.List<org.apache.hadoop.fs.XAttr> listXAttrs(java.lang.String) throws java.io.IOException;

  public abstract void removeXAttr(java.lang.String, org.apache.hadoop.fs.XAttr) throws java.io.IOException;

  public abstract void checkAccess(java.lang.String, org.apache.hadoop.fs.permission.FsAction) throws java.io.IOException;

  public abstract long getCurrentEditLogTxid() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.inotify.EventBatchList getEditsFromTxid(long) throws java.io.IOException;

  public abstract void setErasureCodingPolicy(java.lang.String, java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.AddErasureCodingPolicyResponse[] addErasureCodingPolicies(org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy[]) throws java.io.IOException;

  public abstract void removeErasureCodingPolicy(java.lang.String) throws java.io.IOException;

  public abstract void enableErasureCodingPolicy(java.lang.String) throws java.io.IOException;

  public abstract void disableErasureCodingPolicy(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.ErasureCodingPolicyInfo[] getErasureCodingPolicies() throws java.io.IOException;

  public abstract java.util.Map<java.lang.String, java.lang.String> getErasureCodingCodecs() throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.ErasureCodingPolicy getErasureCodingPolicy(java.lang.String) throws java.io.IOException;

  public abstract void unsetErasureCodingPolicy(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.hdfs.protocol.ECTopologyVerifierResult getECTopologyResultForPolicies(java.lang.String...) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.QuotaUsage getQuotaUsage(java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.OpenFileEntry> listOpenFiles(long) throws java.io.IOException;

  public abstract org.apache.hadoop.fs.BatchedRemoteIterator$BatchedEntries<org.apache.hadoop.hdfs.protocol.OpenFileEntry> listOpenFiles(long, java.util.EnumSet<org.apache.hadoop.hdfs.protocol.OpenFilesIterator$OpenFilesType>, java.lang.String) throws java.io.IOException;

  public abstract org.apache.hadoop.ha.HAServiceProtocol$HAServiceState getHAServiceState() throws java.io.IOException;

  public abstract void msync() throws java.io.IOException;

  public abstract void satisfyStoragePolicy(java.lang.String) throws java.io.IOException;
}
