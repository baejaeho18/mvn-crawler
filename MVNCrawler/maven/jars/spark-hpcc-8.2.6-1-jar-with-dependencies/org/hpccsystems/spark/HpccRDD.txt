Compiled from "HpccRDD.java"
public class org.hpccsystems.spark.HpccRDD extends org.apache.spark.rdd.RDD<org.apache.spark.sql.Row> implements java.io.Serializable {
  private static final long serialVersionUID;

  private static final org.apache.logging.log4j.Logger log;

  private static final scala.reflect.ClassTag<org.apache.spark.sql.Row> CT_RECORD;

  public static int DEFAULT_CONNECTION_TIMEOUT;

  private org.hpccsystems.spark.HpccRDD$InternalPartition[] parts;

  private org.hpccsystems.commons.ecl.FieldDef originalRecordDef;

  private org.hpccsystems.commons.ecl.FieldDef projectedRecordDef;

  private int connectionTimeout;

  private int recordLimit;

  private static void registerPicklingFunctions();
    Code:
       0: invokestatic  #1                  // Method org/apache/spark/sql/execution/python/EvaluatePython.registerPicklers:()V
       3: ldc           #2                  // String pyspark.sql.types
       5: ldc           #3                  // String Row
       7: new           #4                  // class org/hpccsystems/spark/RowConstructor
      10: dup
      11: invokespecial #5                  // Method org/hpccsystems/spark/RowConstructor."<init>":()V
      14: invokestatic  #6                  // Method net/razorvine/pickle/Unpickler.registerConstructor:(Ljava/lang/String;Ljava/lang/String;Lnet/razorvine/pickle/IObjectConstructor;)V
      17: ldc           #2                  // String pyspark.sql.types
      19: ldc           #7                  // String _create_row
      21: new           #4                  // class org/hpccsystems/spark/RowConstructor
      24: dup
      25: invokespecial #5                  // Method org/hpccsystems/spark/RowConstructor."<init>":()V
      28: invokestatic  #6                  // Method net/razorvine/pickle/Unpickler.registerConstructor:(Ljava/lang/String;Ljava/lang/String;Lnet/razorvine/pickle/IObjectConstructor;)V
      31: return

  public org.hpccsystems.spark.HpccRDD(org.apache.spark.SparkContext, org.hpccsystems.dfs.client.DataPartition[], org.hpccsystems.commons.ecl.FieldDef);
    Code:
       0: aload_0
       1: aload_1
       2: aload_2
       3: aload_3
       4: aload_3
       5: invokespecial #8                  // Method "<init>":(Lorg/apache/spark/SparkContext;[Lorg/hpccsystems/dfs/client/DataPartition;Lorg/hpccsystems/commons/ecl/FieldDef;Lorg/hpccsystems/commons/ecl/FieldDef;)V
       8: return

  public org.hpccsystems.spark.HpccRDD(org.apache.spark.SparkContext, org.hpccsystems.dfs.client.DataPartition[], org.hpccsystems.commons.ecl.FieldDef, org.hpccsystems.commons.ecl.FieldDef);
    Code:
       0: aload_0
       1: aload_1
       2: aload_2
       3: aload_3
       4: aload         4
       6: getstatic     #9                  // Field DEFAULT_CONNECTION_TIMEOUT:I
       9: iconst_m1
      10: invokespecial #10                 // Method "<init>":(Lorg/apache/spark/SparkContext;[Lorg/hpccsystems/dfs/client/DataPartition;Lorg/hpccsystems/commons/ecl/FieldDef;Lorg/hpccsystems/commons/ecl/FieldDef;II)V
      13: return

  public org.hpccsystems.spark.HpccRDD(org.apache.spark.SparkContext, org.hpccsystems.dfs.client.DataPartition[], org.hpccsystems.commons.ecl.FieldDef, org.hpccsystems.commons.ecl.FieldDef, int, int);
    Code:
       0: aload_0
       1: aload_1
       2: new           #11                 // class scala/collection/mutable/ArrayBuffer
       5: dup
       6: invokespecial #12                 // Method scala/collection/mutable/ArrayBuffer."<init>":()V
       9: getstatic     #13                 // Field CT_RECORD:Lscala/reflect/ClassTag;
      12: invokespecial #14                 // Method org/apache/spark/rdd/RDD."<init>":(Lorg/apache/spark/SparkContext;Lscala/collection/Seq;Lscala/reflect/ClassTag;)V
      15: aload_0
      16: aconst_null
      17: putfield      #15                 // Field originalRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
      20: aload_0
      21: aconst_null
      22: putfield      #16                 // Field projectedRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
      25: aload_0
      26: getstatic     #9                  // Field DEFAULT_CONNECTION_TIMEOUT:I
      29: putfield      #17                 // Field connectionTimeout:I
      32: aload_0
      33: iconst_m1
      34: putfield      #18                 // Field recordLimit:I
      37: aload_0
      38: aload_2
      39: arraylength
      40: anewarray     #19                 // class org/hpccsystems/spark/HpccRDD$InternalPartition
      43: putfield      #20                 // Field parts:[Lorg/hpccsystems/spark/HpccRDD$InternalPartition;
      46: iconst_0
      47: istore        7
      49: iload         7
      51: aload_2
      52: arraylength
      53: if_icmpge     92
      56: aload_0
      57: getfield      #20                 // Field parts:[Lorg/hpccsystems/spark/HpccRDD$InternalPartition;
      60: iload         7
      62: new           #19                 // class org/hpccsystems/spark/HpccRDD$InternalPartition
      65: dup
      66: aload_0
      67: aconst_null
      68: invokespecial #21                 // Method org/hpccsystems/spark/HpccRDD$InternalPartition."<init>":(Lorg/hpccsystems/spark/HpccRDD;Lorg/hpccsystems/spark/HpccRDD$1;)V
      71: aastore
      72: aload_0
      73: getfield      #20                 // Field parts:[Lorg/hpccsystems/spark/HpccRDD$InternalPartition;
      76: iload         7
      78: aaload
      79: aload_2
      80: iload         7
      82: aaload
      83: putfield      #22                 // Field org/hpccsystems/spark/HpccRDD$InternalPartition.partition:Lorg/hpccsystems/dfs/client/DataPartition;
      86: iinc          7, 1
      89: goto          49
      92: aload_0
      93: aload_3
      94: putfield      #15                 // Field originalRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
      97: aload_0
      98: aload         4
     100: putfield      #16                 // Field projectedRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
     103: aload_0
     104: iload         5
     106: putfield      #17                 // Field connectionTimeout:I
     109: aload_0
     110: iload         6
     112: putfield      #18                 // Field recordLimit:I
     115: return

  public org.apache.spark.api.java.JavaRDD<org.apache.spark.sql.Row> asJavaRDD();
    Code:
       0: new           #23                 // class org/apache/spark/api/java/JavaRDD
       3: dup
       4: aload_0
       5: getstatic     #13                 // Field CT_RECORD:Lscala/reflect/ClassTag;
       8: invokespecial #24                 // Method org/apache/spark/api/java/JavaRDD."<init>":(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;)V
      11: astore_1
      12: aload_1
      13: areturn

  public org.apache.spark.InterruptibleIterator<org.apache.spark.sql.Row> compute(org.apache.spark.Partition, org.apache.spark.TaskContext);
    Code:
       0: invokestatic  #25                 // Method registerPicklingFunctions:()V
       3: aload_1
       4: checkcast     #19                 // class org/hpccsystems/spark/HpccRDD$InternalPartition
       7: astore_3
       8: aload_0
       9: getfield      #15                 // Field originalRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
      12: astore        4
      14: aload_0
      15: getfield      #16                 // Field projectedRecordDef:Lorg/hpccsystems/commons/ecl/FieldDef;
      18: astore        5
      20: aload         4
      22: ifnonnull     37
      25: getstatic     #26                 // Field log:Lorg/apache/logging/log4j/Logger;
      28: ldc           #27                 // String Original record defintion is null. Aborting.
      30: invokeinterface #28,  2           // InterfaceMethod org/apache/logging/log4j/Logger.error:(Ljava/lang/String;)V
      35: aconst_null
      36: areturn
      37: aload         5
      39: ifnonnull     54
      42: getstatic     #26                 // Field log:Lorg/apache/logging/log4j/Logger;
      45: ldc           #29                 // String Projected record defintion is null. Aborting.
      47: invokeinterface #28,  2           // InterfaceMethod org/apache/logging/log4j/Logger.error:(Ljava/lang/String;)V
      52: aconst_null
      53: areturn
      54: aconst_null
      55: astore        6
      57: new           #30                 // class org/hpccsystems/dfs/client/HpccRemoteFileReader
      60: dup
      61: aload_3
      62: getfield      #22                 // Field org/hpccsystems/spark/HpccRDD$InternalPartition.partition:Lorg/hpccsystems/dfs/client/DataPartition;
      65: aload         4
      67: new           #31                 // class org/hpccsystems/spark/GenericRowRecordBuilder
      70: dup
      71: aload         5
      73: invokespecial #32                 // Method org/hpccsystems/spark/GenericRowRecordBuilder."<init>":(Lorg/hpccsystems/commons/ecl/FieldDef;)V
      76: aload_0
      77: getfield      #17                 // Field connectionTimeout:I
      80: aload_0
      81: getfield      #18                 // Field recordLimit:I
      84: invokespecial #33                 // Method org/hpccsystems/dfs/client/HpccRemoteFileReader."<init>":(Lorg/hpccsystems/dfs/client/DataPartition;Lorg/hpccsystems/commons/ecl/FieldDef;Lorg/hpccsystems/dfs/client/IRecordBuilder;II)V
      87: astore        7
      89: aload_2
      90: aload         7
      92: invokedynamic #34,  0             // InvokeDynamic #0:onTaskCompletion:(Lorg/hpccsystems/dfs/client/HpccRemoteFileReader;)Lorg/apache/spark/util/TaskCompletionListener;
      97: invokevirtual #35                 // Method org/apache/spark/TaskContext.addTaskCompletionListener:(Lorg/apache/spark/util/TaskCompletionListener;)Lorg/apache/spark/TaskContext;
     100: pop
     101: aload         7
     103: invokestatic  #36                 // Method scala/collection/JavaConverters.asScalaIteratorConverter:(Ljava/util/Iterator;)Lscala/collection/convert/Decorators$AsScala;
     106: invokevirtual #37                 // Method scala/collection/convert/Decorators$AsScala.asScala:()Ljava/lang/Object;
     109: checkcast     #38                 // class scala/collection/Iterator
     112: astore        6
     114: goto          152
     117: astore        7
     119: getstatic     #26                 // Field log:Lorg/apache/logging/log4j/Logger;
     122: new           #40                 // class java/lang/StringBuilder
     125: dup
     126: invokespecial #41                 // Method java/lang/StringBuilder."<init>":()V
     129: ldc           #42                 // String Failed to create remote file reader with error:
     131: invokevirtual #43                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
     134: aload         7
     136: invokevirtual #44                 // Method java/lang/Exception.getMessage:()Ljava/lang/String;
     139: invokevirtual #43                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
     142: invokevirtual #45                 // Method java/lang/StringBuilder.toString:()Ljava/lang/String;
     145: invokeinterface #28,  2           // InterfaceMethod org/apache/logging/log4j/Logger.error:(Ljava/lang/String;)V
     150: aconst_null
     151: areturn
     152: new           #46                 // class org/apache/spark/InterruptibleIterator
     155: dup
     156: aload_2
     157: aload         6
     159: invokespecial #47                 // Method org/apache/spark/InterruptibleIterator."<init>":(Lorg/apache/spark/TaskContext;Lscala/collection/Iterator;)V
     162: areturn
    Exception table:
       from    to  target type
          57   114   117   Class java/lang/Exception

  public scala.collection.Seq<java.lang.String> getPreferredLocations(org.apache.spark.Partition);
    Code:
       0: aload_1
       1: checkcast     #19                 // class org/hpccsystems/spark/HpccRDD$InternalPartition
       4: astore_2
       5: iconst_1
       6: anewarray     #48                 // class java/lang/String
       9: dup
      10: iconst_0
      11: aload_2
      12: getfield      #22                 // Field org/hpccsystems/spark/HpccRDD$InternalPartition.partition:Lorg/hpccsystems/dfs/client/DataPartition;
      15: invokevirtual #49                 // Method org/hpccsystems/dfs/client/DataPartition.getCopyLocations:()[Ljava/lang/String;
      18: iconst_0
      19: aaload
      20: aastore
      21: invokestatic  #50                 // Method java/util/Arrays.asList:([Ljava/lang/Object;)Ljava/util/List;
      24: invokestatic  #51                 // Method scala/collection/JavaConverters.asScalaBufferConverter:(Ljava/util/List;)Lscala/collection/convert/Decorators$AsScala;
      27: invokevirtual #37                 // Method scala/collection/convert/Decorators$AsScala.asScala:()Ljava/lang/Object;
      30: checkcast     #52                 // class scala/collection/mutable/Buffer
      33: invokeinterface #53,  1           // InterfaceMethod scala/collection/mutable/Buffer.seq:()Lscala/collection/mutable/Seq;
      38: areturn

  public org.apache.spark.Partition[] getPartitions();
    Code:
       0: aload_0
       1: getfield      #20                 // Field parts:[Lorg/hpccsystems/spark/HpccRDD$InternalPartition;
       4: areturn

  public scala.collection.Iterator compute(org.apache.spark.Partition, org.apache.spark.TaskContext);
    Code:
       0: aload_0
       1: aload_1
       2: aload_2
       3: invokevirtual #54                 // Method compute:(Lorg/apache/spark/Partition;Lorg/apache/spark/TaskContext;)Lorg/apache/spark/InterruptibleIterator;
       6: areturn

  private static void lambda$compute$0(org.hpccsystems.dfs.client.HpccRemoteFileReader, org.apache.spark.TaskContext);
    Code:
       0: aload_0
       1: ifnull        12
       4: aload_0
       5: invokevirtual #55                 // Method org/hpccsystems/dfs/client/HpccRemoteFileReader.close:()V
       8: goto          12
      11: astore_2
      12: return
    Exception table:
       from    to  target type
           4     8    11   Class java/lang/Exception

  static {};
    Code:
       0: ldc           #56                 // class org/hpccsystems/spark/HpccRDD
       2: invokestatic  #57                 // Method org/apache/logging/log4j/LogManager.getLogger:(Ljava/lang/Class;)Lorg/apache/logging/log4j/Logger;
       5: putstatic     #26                 // Field log:Lorg/apache/logging/log4j/Logger;
       8: getstatic     #58                 // Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
      11: ldc           #59                 // class org/apache/spark/sql/Row
      13: invokevirtual #60                 // Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
      16: putstatic     #13                 // Field CT_RECORD:Lscala/reflect/ClassTag;
      19: bipush        120
      21: putstatic     #9                  // Field DEFAULT_CONNECTION_TIMEOUT:I
      24: return
}
